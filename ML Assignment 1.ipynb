{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML Assignment 1\n",
    "\n",
    "Claire Wang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. From the L2-regularized cross-entropy loss function given below, derive the formula for a single gradient descent update for $w_k \\in w$. Why would $L1$ be more difficult than calculating the formula for an $L2$ regularized model?\n",
    "\n",
    "\n",
    "Here the L2-regularized Cross Entropy is defined as:\n",
    "$$L=-\\sum y_i ln(p(x_i)) + (1-y_i)ln(1-p(x_i)) + \\lambda ||\\omega||^2$$\n",
    "where $p(x_i)=\\frac{1}{1+e^{-\\omega ' x}}$.\n",
    "\n",
    "Gradient Descent is to minimize the loss function by using its derivative.\n",
    "\n",
    "Derive L2 for a single gradient descent for $w_k \\in w$:\n",
    "\n",
    "$\\frac{\\partial L(W)}{\\partial \\omega_k}= \\frac{\\partial}{\\partial \\omega_k} [-\\sum (y_i ln(\\frac{1}{1+e^{-\\omega x_i}}) + (1-y_i)ln(1-\\frac{1}{1+e^{-\\omega x_i}})) + \\lambda ||\\omega||^2)]$.\n",
    "\n",
    "First derive this part:\n",
    "\n",
    "$-\\frac{\\partial}{\\partial \\omega_k}\\sum (y_i ln(\\frac{1}{1+e^{-\\omega x_i}}) + (1-y_i)ln(1-\\frac{1}{1+e^{-\\omega x_i}}))$\n",
    "\n",
    "$=-\\sum y_i(1+e^{-\\omega x_i}) (-1)(\\frac{1}{1+e^{-\\omega x_i}})^2 e^{-\\omega x_i}(-x_i) + (1-y_i) \\frac{1+e^{-\\omega x_i}}{e^{-\\omega x_i}} (\\frac{1}{1+e^{-\\omega x_i}})^2 e^{-\\omega x_i}(-x_i)$\n",
    "\n",
    "$=-\\sum y_i x_i \\frac{e^{-\\omega x_i}}{1+e^{-\\omega x_i}} - (1-y_i)x_i \\frac{1}{1+e^{-\\omega x_i}}$\n",
    "\n",
    "$=-\\sum y_i x_i (\\frac{e^{-\\omega x_i}}{1+e^{-\\omega x_i}} + \\frac{1}{1+e^{-\\omega x_i}}) - x_i \\frac{1}{1+e^{-\\omega x_i}}$\n",
    "\n",
    "$=\\sum -y_i x_i + x_i p(x_i) = \\sum -(y_i-p(x_i))x_i$\n",
    "\n",
    "\n",
    "then derive the left part:\n",
    "\n",
    "$-\\frac{\\partial}{\\partial \\omega_k} \\lambda ||\\omega||^2 = = -\\frac{\\partial}{\\partial \\omega_k} (\\omega_1^2 + \\omega_2^2 +...+\\omega_k^2 + ...) = -2 \\lambda \\omega_k$\n",
    "\n",
    "therefore:\n",
    "\n",
    "$$\\frac{\\partial L(W)}{\\partial \\omega_k} = \\sum -(y_i-p(x_i))x_i - 2 \\lambda \\omega_k$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "L1 is more difficult than calculating the formula for an L2 regularized model because $\\frac{\\partial}{\\partial \\omega_k} \\lambda \\sum |\\omega_i|$ is more difficult to solve than $\\frac{\\partial}{\\partial \\omega_k} \\lambda\\sum \\omega_i^2$, which leads to sparce solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.  Which logistic regression model (unregularized, L2, or L1) and parameters ($\\lambda$ high or low) may be more useful for determining a small set of parameters that are most important for classification? Why?\n",
    "\n",
    "Lasso tends to do well if there are a small number of significant parameters and the others are close to zero (ergo: when only a few predictors actually influence the response).\n",
    "\n",
    "(Ridge works well if there are many large parameters of about the same value (ergo: when most predictors impact the response).)\n",
    "\n",
    "However, in practice, we don't know the true parameter values, so the previous two points are somewhat theoretical. Just run cross-validation to select the more suited model for a specific case.\n",
    "\n",
    "Also, a low lambda would be better for a small set of parameters that are most important for classification, because we want the coefficients to be larger to be more visible compared to the rest of the parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. miRNA serum\n",
    "\n",
    "\n",
    "1)  Create a L2-regularized logistic regression model to train stochastic gradient descent on the dataset\n",
    " \n",
    "2) Return the cross-entropy loss from at least 40 epochs.  (at least 40 iterations)\n",
    "\n",
    "3) Plot the training and validation loss curves. \n",
    "\n",
    "4) Indicate which iteration gives the best model, and explain your decision. \n",
    "\n",
    "5) Also list and explain your hyperparameter choices (learning rate, regularization parameter)\n",
    " \n",
    " \n",
    "(First try implementing without regularization, then add the regularization terms; also, try different initializations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "X_test = pd.read_csv('X_test.csv',index_col=0)\n",
    "Y_test = pd.read_csv('Y_test.csv',index_col=0, header=None)\n",
    "\n",
    "X_train = pd.read_csv('X_train.csv',index_col=0)\n",
    "Y_train = pd.read_csv('Y_train.csv',index_col=0,header=None)\n",
    "\n",
    "X_val = pd.read_csv('X_val.csv',index_col=0)\n",
    "Y_val = pd.read_csv('Y_val.csv',index_col=0, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>238</th>\n",
       "      <th>239</th>\n",
       "      <th>240</th>\n",
       "      <th>241</th>\n",
       "      <th>242</th>\n",
       "      <th>243</th>\n",
       "      <th>244</th>\n",
       "      <th>245</th>\n",
       "      <th>246</th>\n",
       "      <th>247</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>3354</td>\n",
       "      <td>1.668847</td>\n",
       "      <td>2.815793</td>\n",
       "      <td>4.406506</td>\n",
       "      <td>1.668847</td>\n",
       "      <td>1.668847</td>\n",
       "      <td>1.668847</td>\n",
       "      <td>1.789895</td>\n",
       "      <td>1.668847</td>\n",
       "      <td>1.668847</td>\n",
       "      <td>3.609130</td>\n",
       "      <td>...</td>\n",
       "      <td>1.668847</td>\n",
       "      <td>3.738934</td>\n",
       "      <td>3.323958</td>\n",
       "      <td>7.317184</td>\n",
       "      <td>1.668847</td>\n",
       "      <td>1.668847</td>\n",
       "      <td>1.668847</td>\n",
       "      <td>12.790970</td>\n",
       "      <td>2.124105</td>\n",
       "      <td>1.668847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>843</td>\n",
       "      <td>6.339653</td>\n",
       "      <td>5.224627</td>\n",
       "      <td>5.517739</td>\n",
       "      <td>4.438802</td>\n",
       "      <td>5.280231</td>\n",
       "      <td>4.438802</td>\n",
       "      <td>4.438802</td>\n",
       "      <td>6.089165</td>\n",
       "      <td>4.748844</td>\n",
       "      <td>4.895887</td>\n",
       "      <td>...</td>\n",
       "      <td>4.438802</td>\n",
       "      <td>4.438802</td>\n",
       "      <td>4.438802</td>\n",
       "      <td>6.484823</td>\n",
       "      <td>4.438802</td>\n",
       "      <td>4.438802</td>\n",
       "      <td>4.438802</td>\n",
       "      <td>13.602147</td>\n",
       "      <td>4.438802</td>\n",
       "      <td>4.438802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1536</td>\n",
       "      <td>0.106321</td>\n",
       "      <td>0.106321</td>\n",
       "      <td>4.626492</td>\n",
       "      <td>0.106321</td>\n",
       "      <td>0.106321</td>\n",
       "      <td>0.106321</td>\n",
       "      <td>0.106321</td>\n",
       "      <td>1.297035</td>\n",
       "      <td>0.247058</td>\n",
       "      <td>3.822947</td>\n",
       "      <td>...</td>\n",
       "      <td>0.106321</td>\n",
       "      <td>4.010063</td>\n",
       "      <td>0.106321</td>\n",
       "      <td>7.124066</td>\n",
       "      <td>0.106321</td>\n",
       "      <td>1.138072</td>\n",
       "      <td>0.106321</td>\n",
       "      <td>12.638651</td>\n",
       "      <td>2.678498</td>\n",
       "      <td>0.106321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2986</td>\n",
       "      <td>2.783061</td>\n",
       "      <td>1.757631</td>\n",
       "      <td>4.533316</td>\n",
       "      <td>1.584014</td>\n",
       "      <td>1.584014</td>\n",
       "      <td>2.345445</td>\n",
       "      <td>1.584014</td>\n",
       "      <td>2.141590</td>\n",
       "      <td>1.584014</td>\n",
       "      <td>3.933394</td>\n",
       "      <td>...</td>\n",
       "      <td>2.190110</td>\n",
       "      <td>4.618159</td>\n",
       "      <td>1.584014</td>\n",
       "      <td>7.194896</td>\n",
       "      <td>2.348820</td>\n",
       "      <td>1.584014</td>\n",
       "      <td>3.892735</td>\n",
       "      <td>12.786052</td>\n",
       "      <td>3.761113</td>\n",
       "      <td>3.895201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3609</td>\n",
       "      <td>2.172798</td>\n",
       "      <td>2.172798</td>\n",
       "      <td>4.608753</td>\n",
       "      <td>2.918387</td>\n",
       "      <td>3.245393</td>\n",
       "      <td>2.172798</td>\n",
       "      <td>3.369072</td>\n",
       "      <td>2.172798</td>\n",
       "      <td>2.172798</td>\n",
       "      <td>3.425787</td>\n",
       "      <td>...</td>\n",
       "      <td>2.172798</td>\n",
       "      <td>4.465491</td>\n",
       "      <td>2.172798</td>\n",
       "      <td>7.018009</td>\n",
       "      <td>3.810020</td>\n",
       "      <td>2.172798</td>\n",
       "      <td>2.710400</td>\n",
       "      <td>12.975085</td>\n",
       "      <td>2.841749</td>\n",
       "      <td>2.294369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>572</td>\n",
       "      <td>6.372373</td>\n",
       "      <td>6.372373</td>\n",
       "      <td>6.372373</td>\n",
       "      <td>6.372373</td>\n",
       "      <td>6.372373</td>\n",
       "      <td>6.372373</td>\n",
       "      <td>6.372373</td>\n",
       "      <td>6.372373</td>\n",
       "      <td>6.372373</td>\n",
       "      <td>6.372373</td>\n",
       "      <td>...</td>\n",
       "      <td>6.372373</td>\n",
       "      <td>6.372373</td>\n",
       "      <td>6.372373</td>\n",
       "      <td>6.620049</td>\n",
       "      <td>6.372373</td>\n",
       "      <td>6.372373</td>\n",
       "      <td>6.372373</td>\n",
       "      <td>13.200571</td>\n",
       "      <td>6.372373</td>\n",
       "      <td>6.372373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>6.051375</td>\n",
       "      <td>3.801965</td>\n",
       "      <td>5.054876</td>\n",
       "      <td>2.539181</td>\n",
       "      <td>3.857434</td>\n",
       "      <td>2.539181</td>\n",
       "      <td>2.733472</td>\n",
       "      <td>4.661771</td>\n",
       "      <td>2.539181</td>\n",
       "      <td>4.275232</td>\n",
       "      <td>...</td>\n",
       "      <td>2.539181</td>\n",
       "      <td>2.539181</td>\n",
       "      <td>2.539181</td>\n",
       "      <td>6.062675</td>\n",
       "      <td>2.539181</td>\n",
       "      <td>2.539181</td>\n",
       "      <td>2.539181</td>\n",
       "      <td>12.368748</td>\n",
       "      <td>3.007394</td>\n",
       "      <td>3.358611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3646</td>\n",
       "      <td>3.475436</td>\n",
       "      <td>3.022719</td>\n",
       "      <td>4.471867</td>\n",
       "      <td>3.022719</td>\n",
       "      <td>3.022719</td>\n",
       "      <td>3.022719</td>\n",
       "      <td>3.022719</td>\n",
       "      <td>3.022719</td>\n",
       "      <td>3.022719</td>\n",
       "      <td>3.696326</td>\n",
       "      <td>...</td>\n",
       "      <td>3.022719</td>\n",
       "      <td>4.063070</td>\n",
       "      <td>3.022719</td>\n",
       "      <td>6.852801</td>\n",
       "      <td>3.235907</td>\n",
       "      <td>3.022719</td>\n",
       "      <td>3.022719</td>\n",
       "      <td>12.578440</td>\n",
       "      <td>3.374728</td>\n",
       "      <td>3.022719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3290</td>\n",
       "      <td>1.798023</td>\n",
       "      <td>1.798023</td>\n",
       "      <td>4.263286</td>\n",
       "      <td>1.798023</td>\n",
       "      <td>1.798023</td>\n",
       "      <td>1.798023</td>\n",
       "      <td>1.798023</td>\n",
       "      <td>1.798023</td>\n",
       "      <td>1.798023</td>\n",
       "      <td>2.233266</td>\n",
       "      <td>...</td>\n",
       "      <td>1.798023</td>\n",
       "      <td>3.922277</td>\n",
       "      <td>1.798023</td>\n",
       "      <td>6.534373</td>\n",
       "      <td>2.061887</td>\n",
       "      <td>1.798023</td>\n",
       "      <td>1.798023</td>\n",
       "      <td>12.577388</td>\n",
       "      <td>1.798023</td>\n",
       "      <td>1.798023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1316</td>\n",
       "      <td>1.260822</td>\n",
       "      <td>1.260822</td>\n",
       "      <td>4.327651</td>\n",
       "      <td>2.248170</td>\n",
       "      <td>1.260822</td>\n",
       "      <td>2.239181</td>\n",
       "      <td>1.408976</td>\n",
       "      <td>1.260822</td>\n",
       "      <td>1.763213</td>\n",
       "      <td>3.816110</td>\n",
       "      <td>...</td>\n",
       "      <td>1.260822</td>\n",
       "      <td>4.329167</td>\n",
       "      <td>1.260822</td>\n",
       "      <td>7.590831</td>\n",
       "      <td>1.739990</td>\n",
       "      <td>1.260822</td>\n",
       "      <td>2.066584</td>\n",
       "      <td>12.548400</td>\n",
       "      <td>1.260822</td>\n",
       "      <td>1.260822</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1325 rows × 247 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             1         2         3         4         5         6         7  \\\n",
       "3354  1.668847  2.815793  4.406506  1.668847  1.668847  1.668847  1.789895   \n",
       "843   6.339653  5.224627  5.517739  4.438802  5.280231  4.438802  4.438802   \n",
       "1536  0.106321  0.106321  4.626492  0.106321  0.106321  0.106321  0.106321   \n",
       "2986  2.783061  1.757631  4.533316  1.584014  1.584014  2.345445  1.584014   \n",
       "3609  2.172798  2.172798  4.608753  2.918387  3.245393  2.172798  3.369072   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "572   6.372373  6.372373  6.372373  6.372373  6.372373  6.372373  6.372373   \n",
       "720   6.051375  3.801965  5.054876  2.539181  3.857434  2.539181  2.733472   \n",
       "3646  3.475436  3.022719  4.471867  3.022719  3.022719  3.022719  3.022719   \n",
       "3290  1.798023  1.798023  4.263286  1.798023  1.798023  1.798023  1.798023   \n",
       "1316  1.260822  1.260822  4.327651  2.248170  1.260822  2.239181  1.408976   \n",
       "\n",
       "             8         9        10  ...       238       239       240  \\\n",
       "3354  1.668847  1.668847  3.609130  ...  1.668847  3.738934  3.323958   \n",
       "843   6.089165  4.748844  4.895887  ...  4.438802  4.438802  4.438802   \n",
       "1536  1.297035  0.247058  3.822947  ...  0.106321  4.010063  0.106321   \n",
       "2986  2.141590  1.584014  3.933394  ...  2.190110  4.618159  1.584014   \n",
       "3609  2.172798  2.172798  3.425787  ...  2.172798  4.465491  2.172798   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "572   6.372373  6.372373  6.372373  ...  6.372373  6.372373  6.372373   \n",
       "720   4.661771  2.539181  4.275232  ...  2.539181  2.539181  2.539181   \n",
       "3646  3.022719  3.022719  3.696326  ...  3.022719  4.063070  3.022719   \n",
       "3290  1.798023  1.798023  2.233266  ...  1.798023  3.922277  1.798023   \n",
       "1316  1.260822  1.763213  3.816110  ...  1.260822  4.329167  1.260822   \n",
       "\n",
       "           241       242       243       244        245       246       247  \n",
       "3354  7.317184  1.668847  1.668847  1.668847  12.790970  2.124105  1.668847  \n",
       "843   6.484823  4.438802  4.438802  4.438802  13.602147  4.438802  4.438802  \n",
       "1536  7.124066  0.106321  1.138072  0.106321  12.638651  2.678498  0.106321  \n",
       "2986  7.194896  2.348820  1.584014  3.892735  12.786052  3.761113  3.895201  \n",
       "3609  7.018009  3.810020  2.172798  2.710400  12.975085  2.841749  2.294369  \n",
       "...        ...       ...       ...       ...        ...       ...       ...  \n",
       "572   6.620049  6.372373  6.372373  6.372373  13.200571  6.372373  6.372373  \n",
       "720   6.062675  2.539181  2.539181  2.539181  12.368748  3.007394  3.358611  \n",
       "3646  6.852801  3.235907  3.022719  3.022719  12.578440  3.374728  3.022719  \n",
       "3290  6.534373  2.061887  1.798023  1.798023  12.577388  1.798023  1.798023  \n",
       "1316  7.590831  1.739990  1.260822  2.066584  12.548400  1.260822  1.260822  \n",
       "\n",
       "[1325 rows x 247 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>3354</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>843</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1536</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2986</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3609</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>572</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3646</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3290</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1316</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1325 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      1\n",
       "0      \n",
       "3354  0\n",
       "843   1\n",
       "1536  0\n",
       "2986  0\n",
       "3609  0\n",
       "...  ..\n",
       "572   1\n",
       "720   1\n",
       "3646  0\n",
       "3290  0\n",
       "1316  0\n",
       "\n",
       "[1325 rows x 1 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 3.88138364e-01],\n",
       "       [ 9.93601562e-03],\n",
       "       [ 6.03898684e-02],\n",
       "       [-3.23842781e-01],\n",
       "       [ 1.12741639e-01],\n",
       "       [-1.68655085e-01],\n",
       "       [ 7.84803594e-02],\n",
       "       [-2.42603166e-02],\n",
       "       [ 7.84699499e-02],\n",
       "       [ 5.27818578e-01],\n",
       "       [-4.09657351e-01],\n",
       "       [ 3.48702288e-01],\n",
       "       [ 2.86988961e-01],\n",
       "       [ 2.89848723e-01],\n",
       "       [-1.14526372e+00],\n",
       "       [-2.33625423e-01],\n",
       "       [ 6.51030391e-01],\n",
       "       [ 6.67092240e-01],\n",
       "       [-3.59465776e-01],\n",
       "       [-1.83338508e-01],\n",
       "       [-6.06156652e-03],\n",
       "       [-3.24090324e-01],\n",
       "       [-5.69021240e-01],\n",
       "       [-1.76546557e-01],\n",
       "       [-5.24257464e-01],\n",
       "       [-2.60065789e-01],\n",
       "       [-2.45493565e-02],\n",
       "       [-2.22508026e-01],\n",
       "       [ 3.06120344e-01],\n",
       "       [-4.15210789e-01],\n",
       "       [ 1.43653296e-01],\n",
       "       [-1.77045895e-01],\n",
       "       [ 1.79623598e-01],\n",
       "       [ 2.02098910e-01],\n",
       "       [ 4.44778831e-01],\n",
       "       [ 1.41927602e-01],\n",
       "       [-3.73813919e-01],\n",
       "       [ 6.92751141e-01],\n",
       "       [ 1.61272303e-02],\n",
       "       [ 9.43189972e-02],\n",
       "       [ 1.12715029e-01],\n",
       "       [ 7.37648907e-01],\n",
       "       [-1.45746993e-01],\n",
       "       [ 1.17930870e+00],\n",
       "       [ 2.31250144e-01],\n",
       "       [ 4.97801652e-01],\n",
       "       [-6.08070315e-01],\n",
       "       [ 1.11648852e+00],\n",
       "       [-3.10125609e-01],\n",
       "       [-2.69365592e-01],\n",
       "       [-6.07756386e-02],\n",
       "       [ 7.78985085e-01],\n",
       "       [-2.40854348e-01],\n",
       "       [ 1.72977598e-01],\n",
       "       [ 4.26883742e-01],\n",
       "       [-3.40606356e-01],\n",
       "       [ 7.25605077e-02],\n",
       "       [ 6.14590972e-01],\n",
       "       [-1.53014207e-01],\n",
       "       [ 4.14102064e-01],\n",
       "       [ 3.32011391e-01],\n",
       "       [-3.86201239e-01],\n",
       "       [ 3.00622952e-01],\n",
       "       [-2.23958286e-01],\n",
       "       [-1.51899247e-01],\n",
       "       [-3.34184530e-01],\n",
       "       [-2.21831482e-01],\n",
       "       [-1.13087493e-01],\n",
       "       [ 1.24958788e-02],\n",
       "       [-7.24480184e-02],\n",
       "       [ 4.15943830e-01],\n",
       "       [ 2.61269975e-01],\n",
       "       [ 1.70421819e-01],\n",
       "       [-2.29249598e-01],\n",
       "       [ 1.13207453e-01],\n",
       "       [ 1.74004961e-01],\n",
       "       [ 4.91431558e-01],\n",
       "       [-1.44851429e-01],\n",
       "       [ 7.75174914e-02],\n",
       "       [-2.35624909e+00],\n",
       "       [ 3.34385852e-02],\n",
       "       [-4.68030775e-01],\n",
       "       [ 3.41883563e-01],\n",
       "       [-1.04579795e-01],\n",
       "       [ 1.38488282e-01],\n",
       "       [-5.43467181e-01],\n",
       "       [ 7.45713670e-01],\n",
       "       [-1.38641217e-01],\n",
       "       [ 1.79467394e-01],\n",
       "       [ 2.45900091e-01],\n",
       "       [-6.67038793e-01],\n",
       "       [ 1.15890320e-02],\n",
       "       [-7.68969439e-01],\n",
       "       [-3.42831857e-01],\n",
       "       [-3.86500240e-01],\n",
       "       [-9.79976865e-01],\n",
       "       [ 7.99363160e-02],\n",
       "       [ 4.71339583e-02],\n",
       "       [ 1.74452836e-01],\n",
       "       [-1.06805567e-01],\n",
       "       [ 4.47302834e-01],\n",
       "       [-3.69850334e-01],\n",
       "       [ 7.98927057e-01],\n",
       "       [ 5.04143035e-01],\n",
       "       [ 2.42296949e-01],\n",
       "       [ 2.15361595e-01],\n",
       "       [ 5.01301171e-02],\n",
       "       [ 5.87524038e-01],\n",
       "       [-5.43737626e-02],\n",
       "       [-8.66403323e-02],\n",
       "       [ 3.24405834e-02],\n",
       "       [-4.67226581e-01],\n",
       "       [-1.12521491e-01],\n",
       "       [-2.33505315e-02],\n",
       "       [-5.19127226e-01],\n",
       "       [ 3.12722721e-01],\n",
       "       [ 2.41615117e-01],\n",
       "       [ 8.35236324e-02],\n",
       "       [ 6.25701559e-02],\n",
       "       [-6.59883863e-01],\n",
       "       [-1.81845966e-01],\n",
       "       [ 1.01208117e+00],\n",
       "       [ 6.17142887e-01],\n",
       "       [ 1.82395159e-01],\n",
       "       [ 2.41073733e-01],\n",
       "       [-6.40710739e-01],\n",
       "       [ 3.51841884e-02],\n",
       "       [ 9.53359153e-01],\n",
       "       [ 1.41608651e-01],\n",
       "       [ 3.42410957e-03],\n",
       "       [ 7.91451936e-01],\n",
       "       [ 2.59169434e-02],\n",
       "       [-1.02461200e-01],\n",
       "       [ 2.61747810e-01],\n",
       "       [-2.29514941e-01],\n",
       "       [-6.64926982e-02],\n",
       "       [-1.21343927e-01],\n",
       "       [-2.24874986e-01],\n",
       "       [ 4.74664304e-01],\n",
       "       [-4.62945811e-01],\n",
       "       [-1.87234901e-01],\n",
       "       [ 3.87097669e-01],\n",
       "       [-1.28290198e-01],\n",
       "       [ 3.42143434e-01],\n",
       "       [-1.32081601e-01],\n",
       "       [ 1.33908011e-01],\n",
       "       [ 1.84028676e-01],\n",
       "       [-9.77550274e-02],\n",
       "       [-5.96203483e-02],\n",
       "       [ 3.52639904e-01],\n",
       "       [ 1.83238135e-01],\n",
       "       [ 4.76917372e-01],\n",
       "       [ 3.10256809e-01],\n",
       "       [ 6.37662523e-01],\n",
       "       [-1.22876612e-01],\n",
       "       [-1.36038969e-01],\n",
       "       [ 2.17075452e-01],\n",
       "       [ 1.38056727e-01],\n",
       "       [-3.52231282e-01],\n",
       "       [ 6.55120429e-02],\n",
       "       [ 3.03478386e-01],\n",
       "       [ 1.28754187e-01],\n",
       "       [-2.10084483e-01],\n",
       "       [-5.68646774e-01],\n",
       "       [ 1.72000445e-01],\n",
       "       [ 9.88744885e-01],\n",
       "       [ 1.46858369e-02],\n",
       "       [ 7.99105952e-01],\n",
       "       [ 3.58982398e-03],\n",
       "       [-1.36353561e-01],\n",
       "       [-7.63282786e-01],\n",
       "       [-1.95677499e-01],\n",
       "       [ 3.66555253e-01],\n",
       "       [ 5.76431236e-02],\n",
       "       [-4.12200548e-01],\n",
       "       [ 2.65529727e-01],\n",
       "       [-1.46577299e-01],\n",
       "       [ 3.36702948e-02],\n",
       "       [-2.00194813e-01],\n",
       "       [ 5.27994646e-01],\n",
       "       [-1.55553941e-01],\n",
       "       [-2.35429827e-01],\n",
       "       [-1.75756592e-01],\n",
       "       [-4.97613565e-01],\n",
       "       [-1.26413452e-01],\n",
       "       [ 7.86978670e-02],\n",
       "       [ 2.84548094e-01],\n",
       "       [ 6.92512808e-01],\n",
       "       [ 2.39831496e-01],\n",
       "       [-6.27386843e-01],\n",
       "       [-2.09599816e-01],\n",
       "       [-4.92197271e-01],\n",
       "       [ 4.00487227e-01],\n",
       "       [-2.97837795e-01],\n",
       "       [ 2.27717235e-01],\n",
       "       [ 2.57484143e-01],\n",
       "       [ 1.26064268e-01],\n",
       "       [-9.02779007e-03],\n",
       "       [ 2.28012523e-01],\n",
       "       [ 1.92553192e-02],\n",
       "       [-8.81511121e-01],\n",
       "       [-4.82981858e-01],\n",
       "       [-2.22892294e-02],\n",
       "       [-5.21117976e-01],\n",
       "       [-1.45982006e-01],\n",
       "       [-1.40176657e-01],\n",
       "       [-1.88041704e-01],\n",
       "       [-1.92684109e-02],\n",
       "       [-2.73330846e-01],\n",
       "       [-1.73681745e-01],\n",
       "       [ 3.54969842e-01],\n",
       "       [ 8.67441700e-02],\n",
       "       [ 2.71412866e-02],\n",
       "       [-2.66283311e-01],\n",
       "       [-7.23188226e-01],\n",
       "       [-1.63616829e-03],\n",
       "       [-6.78566180e-01],\n",
       "       [-3.02854924e-02],\n",
       "       [-4.23454135e-01],\n",
       "       [-9.11090926e-02],\n",
       "       [-1.72366104e-01],\n",
       "       [-5.01732328e-01],\n",
       "       [-1.70354069e-01],\n",
       "       [ 7.82262198e-01],\n",
       "       [ 1.31984425e+00],\n",
       "       [ 5.74329993e-02],\n",
       "       [-2.30301345e-01],\n",
       "       [-1.22078982e-02],\n",
       "       [ 8.48577427e-01],\n",
       "       [-2.52450955e-01],\n",
       "       [-2.20877586e-01],\n",
       "       [-5.05776753e-01],\n",
       "       [ 6.54760370e-01],\n",
       "       [-2.16449889e-01],\n",
       "       [-9.11069495e-03],\n",
       "       [-2.84334929e-01],\n",
       "       [-3.98850231e-01],\n",
       "       [ 2.78420391e-01],\n",
       "       [ 9.32130721e-04],\n",
       "       [-1.68843017e-02],\n",
       "       [-7.97899036e-01],\n",
       "       [-9.20595694e-01],\n",
       "       [ 1.28238183e-02],\n",
       "       [ 5.22878901e-01],\n",
       "       [ 6.19948529e-01],\n",
       "       [-4.26528577e-01],\n",
       "       [-1.03826282e-01]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to get weights (coefficients) of the data:\n",
    "#Y_train = Y_train.value.ravel()\n",
    "#Y_val = Y_val.value.ravel()\n",
    "#Y_test = Y_test.value.ravel()\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "clf_train = LogisticRegression()\n",
    "clf_train.fit(X_train,Y_train)\n",
    "\n",
    "coef_train= clf_train.coef_\n",
    "coef_train= coef_train.reshape(247,1)\n",
    "\n",
    "clf_val = LogisticRegression()\n",
    "clf_val.fit(X_val,Y_val)\n",
    "coef_val = clf_val.coef_\n",
    "\n",
    "coef_val = coef_val.reshape(247,1)\n",
    "coef_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1325, 247)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1325, 1)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(247, 1)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coef_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "  return 1.0 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For logistic regression, use the cost function as the error function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(features, weights): \n",
    "  '''\n",
    "  Returns 1D array of probabilities\n",
    "  that the class label == 1\n",
    "  '''\n",
    "  z = np.dot(features, weights)\n",
    "  return sigmoid(z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[11.399498  ],\n",
       "       [-6.18028163],\n",
       "       [ 4.53326649],\n",
       "       ...,\n",
       "       [ 6.98245595],\n",
       "       [-6.84736853],\n",
       "       [-4.07950437]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(X_train, coef_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.9999888 ],\n",
       "       [0.00206557],\n",
       "       [0.98936872],\n",
       "       ...,\n",
       "       [0.99907284],\n",
       "       [0.00106112],\n",
       "       [0.01663446]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(X_train, coef_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_function(features, labels, weights):\n",
    "    '''\n",
    "    Using Mean Absolute Error\n",
    "\n",
    "    Features:(1325,247)\n",
    "    Labels: (1325,1)\n",
    "    Weights:(247,1)\n",
    "    Returns 1D matrix of predictions\n",
    "    Cost = (labels*log(predictions) + (1-labels)*log(1-predictions) ) / len(labels)\n",
    "    '''\n",
    "    observations = len(labels)\n",
    "\n",
    "    predictions = predict(features, weights)\n",
    "\n",
    "    #Take the error when label=1\n",
    "    class1_cost = -labels*np.log(predictions) # to kill RuntimeWarning: divide by zero encountered in log\n",
    "\n",
    "    #Take the error when label=0\n",
    "    class2_cost = (1-labels)*np.log(1-predictions)\n",
    "\n",
    "    #Take the sum of both costs\n",
    "    cost = class1_cost - class2_cost\n",
    "\n",
    "    #Take the average cost\n",
    "    cost = cost.sum() / observations\n",
    "\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Define a function to calculate the stochastic gradient update:\n",
    "$$\\omega_k'=\\omega_k + \\alpha [\\sum_i^N(y_i-p(x_i))x_i+2\\lambda \\omega_k]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_weights(features, labels, weights, lr, rp): # lr= learning rate, rp = regularization parameter\n",
    "    '''\n",
    "    Vectorized Gradient Descent\n",
    "\n",
    "    Features:(1325,247)\n",
    "    Labels: (1325,1)\n",
    "    Weights:(247,1)\n",
    "    '''\n",
    "    N = len(features)\n",
    "\n",
    "    #1 - Get Predictions\n",
    "    predictions = predict(features, weights)\n",
    "\n",
    "    gradient = np.dot(features.T,  predictions - labels)\n",
    "\n",
    "    #3 Take the average cost derivative for each feature\n",
    "    gradient /= N\n",
    "    \n",
    "    # Regularized cost function = cost function + P\n",
    "    gradient += 2 * rp * weights\n",
    "\n",
    "    #4 - Multiply the gradient by our learning rate\n",
    "    gradient *= lr\n",
    "\n",
    "    #5 - Subtract from our weights to minimize cost\n",
    "    weights += gradient\n",
    "\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(features, labels, weights, lr, rp, iters): \n",
    "    ite=[]\n",
    "    cost_history= []\n",
    "    for i in range(iters):\n",
    "        weights = update_weights(features, labels, weights, lr, rp)\n",
    "        #Calculate error for auditing purposes\n",
    "        cost = cost_function(features, labels, weights)\n",
    "        cost_history.append(cost)\n",
    "        ite.append(i)\n",
    "\n",
    "    return ite, cost_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we feed in the data, learning rate and regularization parameters. We first determine which regularization parameter to use by comparing the accuracy score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Testing Accuracy:  0.9119448698315467\n",
      "Mean Testing Accuracy:  0.9173047473200613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Testing Accuracy:  0.9387442572741195\n",
      "Mean Testing Accuracy:  0.9594180704441041\n",
      "Mean Testing Accuracy:  0.9693721286370597\n",
      "Mean Testing Accuracy:  0.9532924961715161\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "rp = [0.001, 0.01, 0.1, 1, 10, 100]\n",
    "#Y_train = Y_train.values.ravel()\n",
    "#Y_test = Y_test.values.ravel()\n",
    "for i in rp:\n",
    "    model = LogisticRegression(penalty=\"l2\",C=1/i,solver='liblinear')\n",
    "    model.fit(X_train, Y_train)\n",
    "    print('Mean Testing Accuracy: ', model.score(X_test,Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the highest accuracy 0.9693721286370597, we choose rp = 10. I choose learning rate to be 0.001, because if it is too high, after a certain number of iterations, the cost value becomes infinite (tried from 1, 0.01, down to 0.001)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:19: RuntimeWarning: divide by zero encountered in log\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([0,\n",
       "  1,\n",
       "  2,\n",
       "  3,\n",
       "  4,\n",
       "  5,\n",
       "  6,\n",
       "  7,\n",
       "  8,\n",
       "  9,\n",
       "  10,\n",
       "  11,\n",
       "  12,\n",
       "  13,\n",
       "  14,\n",
       "  15,\n",
       "  16,\n",
       "  17,\n",
       "  18,\n",
       "  19,\n",
       "  20,\n",
       "  21,\n",
       "  22,\n",
       "  23,\n",
       "  24,\n",
       "  25,\n",
       "  26,\n",
       "  27,\n",
       "  28,\n",
       "  29,\n",
       "  30,\n",
       "  31,\n",
       "  32,\n",
       "  33,\n",
       "  34,\n",
       "  35,\n",
       "  36,\n",
       "  37,\n",
       "  38,\n",
       "  39,\n",
       "  40],\n",
       " [1    0.047566\n",
       "  dtype: float64, 1    0.046978\n",
       "  dtype: float64, 1    0.046429\n",
       "  dtype: float64, 1    0.045918\n",
       "  dtype: float64, 1    0.045444\n",
       "  dtype: float64, 1    0.045006\n",
       "  dtype: float64, 1    0.044604\n",
       "  dtype: float64, 1    0.044237\n",
       "  dtype: float64, 1    0.043904\n",
       "  dtype: float64, 1    0.043606\n",
       "  dtype: float64, 1    0.04334\n",
       "  dtype: float64, 1    0.043107\n",
       "  dtype: float64, 1    0.042906\n",
       "  dtype: float64, 1    0.042737\n",
       "  dtype: float64, 1    0.042598\n",
       "  dtype: float64, 1    0.04249\n",
       "  dtype: float64, 1    0.042412\n",
       "  dtype: float64, 1    0.042363\n",
       "  dtype: float64, 1    0.042343\n",
       "  dtype: float64, 1    0.042352\n",
       "  dtype: float64, 1    0.042388\n",
       "  dtype: float64, 1    0.042453\n",
       "  dtype: float64, 1    0.042544\n",
       "  dtype: float64, 1    0.042663\n",
       "  dtype: float64, 1    0.042809\n",
       "  dtype: float64, 1    0.04298\n",
       "  dtype: float64, 1    0.043178\n",
       "  dtype: float64, 1    0.043402\n",
       "  dtype: float64, 1    0.043651\n",
       "  dtype: float64, 1    0.043925\n",
       "  dtype: float64, 1    0.044225\n",
       "  dtype: float64, 1    0.04455\n",
       "  dtype: float64, 1    0.0449\n",
       "  dtype: float64, 1    0.045274\n",
       "  dtype: float64, 1    0.045673\n",
       "  dtype: float64, 1    0.046097\n",
       "  dtype: float64, 1    0.046545\n",
       "  dtype: float64, 1    0.047018\n",
       "  dtype: float64, 1    0.047515\n",
       "  dtype: float64, 1    0.048037\n",
       "  dtype: float64, 1    0.048584\n",
       "  dtype: float64])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train(features=X_train,labels=Y_train, weights=coef_train, lr=0.001, rp=10, iters=41)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "ite_train = list(range(41))\n",
    "cost_train = [0.047566,0.046978,0.046429,0.045918,0.045444,0.045006,0.044604,0.044237,0.043904,0.043606,0.04334,0.043107,0.042906\n",
    ",0.042737,0.042598,0.04249,0.042412,0.042363,0.042343,0.042352,0.042388,0.042453,0.042544,0.042663,0.042809,0.04298\n",
    ",0.043178,0.043402,0.043651,0.043925,0.044225,0.04455,0.0449,0.045274,0.045673,0.046097,0.046545,0.047018,0.047515\n",
    ",0.048037,0.048584]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:19: RuntimeWarning: divide by zero encountered in log\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([0,\n",
       "  1,\n",
       "  2,\n",
       "  3,\n",
       "  4,\n",
       "  5,\n",
       "  6,\n",
       "  7,\n",
       "  8,\n",
       "  9,\n",
       "  10,\n",
       "  11,\n",
       "  12,\n",
       "  13,\n",
       "  14,\n",
       "  15,\n",
       "  16,\n",
       "  17,\n",
       "  18,\n",
       "  19,\n",
       "  20,\n",
       "  21,\n",
       "  22,\n",
       "  23,\n",
       "  24,\n",
       "  25,\n",
       "  26,\n",
       "  27,\n",
       "  28,\n",
       "  29,\n",
       "  30,\n",
       "  31,\n",
       "  32,\n",
       "  33,\n",
       "  34,\n",
       "  35,\n",
       "  36,\n",
       "  37,\n",
       "  38,\n",
       "  39,\n",
       "  40],\n",
       " [1    0.048139\n",
       "  dtype: float64, 1    0.047518\n",
       "  dtype: float64, 1    0.046939\n",
       "  dtype: float64, 1    0.046403\n",
       "  dtype: float64, 1    0.045907\n",
       "  dtype: float64, 1    0.045452\n",
       "  dtype: float64, 1    0.045036\n",
       "  dtype: float64, 1    0.044659\n",
       "  dtype: float64, 1    0.04432\n",
       "  dtype: float64, 1    0.044018\n",
       "  dtype: float64, 1    0.043753\n",
       "  dtype: float64, 1    0.043523\n",
       "  dtype: float64, 1    0.043328\n",
       "  dtype: float64, 1    0.043167\n",
       "  dtype: float64, 1    0.043039\n",
       "  dtype: float64, 1    0.042945\n",
       "  dtype: float64, 1    0.042882\n",
       "  dtype: float64, 1    0.042852\n",
       "  dtype: float64, 1    0.042852\n",
       "  dtype: float64, 1    0.042882\n",
       "  dtype: float64, 1    0.042942\n",
       "  dtype: float64, 1    0.043032\n",
       "  dtype: float64, 1    0.04315\n",
       "  dtype: float64, 1    0.043296\n",
       "  dtype: float64, 1    0.043471\n",
       "  dtype: float64, 1    0.043672\n",
       "  dtype: float64, 1    0.043901\n",
       "  dtype: float64, 1    0.044156\n",
       "  dtype: float64, 1    0.044437\n",
       "  dtype: float64, 1    0.044745\n",
       "  dtype: float64, 1    0.045077\n",
       "  dtype: float64, 1    0.045436\n",
       "  dtype: float64, 1    0.045819\n",
       "  dtype: float64, 1    0.046227\n",
       "  dtype: float64, 1    0.04666\n",
       "  dtype: float64, 1    0.047117\n",
       "  dtype: float64, 1    0.047598\n",
       "  dtype: float64, 1    0.048104\n",
       "  dtype: float64, 1    0.048633\n",
       "  dtype: float64, 1    0.049187\n",
       "  dtype: float64, 1    0.049764\n",
       "  dtype: float64])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train(features=X_val,labels=Y_val, weights=coef_val, lr=0.001, rp=10, iters=41)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I don't know why there's no ite and cost_history from the train() function, which only allows me to return these two values, so I generate them manually..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "ite_val = list(range(41))\n",
    "cost_val = [0.048139,0.047518,0.046939,0.046403,0.045907,0.045452,0.045036,0.044659,0.04432,0.044018,0.043753\n",
    ",0.043523,0.043328,0.043167,0.043039,0.042945,0.042882,0.042852,0.042852,0.042882,0.042942,0.043032,0.04315,0.043296\n",
    ",0.043471,0.043672,0.043901,0.044156,0.044437,0.044745,0.045077,0.045436,0.045819,0.046227,0.04666,0.047117\n",
    ",0.047598,0.048104,0.048633,0.049187,0.049764]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(ite_train, cost_train, label=\"Train data set\")\n",
    "plt.plot(ite_val, cost_val, label='Validation data set')\n",
    "plt.xlabel('Ephochs')\n",
    "plt.ylabel('Cross-entropy Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimum model complexity is in the middle of the curve. At ephochs around 10 to 28, the cost error is smaller. Specifically, under approximately 18 iterations, the model is the best."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. \n",
    "### Train a logistic regression on the data set with lambda = 1e-3,1e-2, 1e-1, 1, 10, 100, using L2, L1, and Elastic net regularizations on the training and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted Labels:  [1 0 1 0 0 1 0 0 0 0]\n",
      "True Testing Labels:  [1 0 1 0 0 1 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# Take \"L1 model, lambda = 0.001, train data\" as an example:\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "#Y_train = Y_train.values.ravel()\n",
    "#Y_val = Y_val.values.ravel()\n",
    "#Y_test = Y_test.values.ravel()\n",
    "\n",
    "\n",
    "lambda_=0.001\n",
    "c=1/lambda_\n",
    "lasso_model = LogisticRegression(penalty=\"l1\",C=c,solver='liblinear')\n",
    "lasso_model.fit(X_train, Y_train)\n",
    "y_pred = lasso_model.predict(X_train)\n",
    "\n",
    "print(\"predicted Labels: \",y_pred[0:10])\n",
    "print(\"True Testing Labels: \",Y_train[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 Testing Predictions: \n",
      "[Prob = 1 Prob = 0]\n",
      "[[0.00000000e+00 1.00000000e+00]\n",
      " [9.98061665e-01 1.93833545e-03]\n",
      " [4.16276530e-06 9.99995837e-01]\n",
      " [1.00000000e+00 3.88370364e-22]\n",
      " [9.96932247e-01 3.06775328e-03]\n",
      " [1.17990760e-04 9.99882009e-01]\n",
      " [9.99980676e-01 1.93243367e-05]\n",
      " [9.82528754e-01 1.74712463e-02]\n",
      " [9.99999996e-01 3.61987554e-09]\n",
      " [1.00000000e+00 1.33456598e-23]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "X has 3 features per sample; expecting 247",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-98-cedff0d1757f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"[Prob = 1\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Prob = 0]\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlasso_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Mean Testing Accuracy: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlasso_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36mscore\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    355\u001b[0m         \"\"\"\n\u001b[1;32m    356\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/base.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    287\u001b[0m             \u001b[0mPredicted\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0mper\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m         \"\"\"\n\u001b[0;32m--> 289\u001b[0;31m         \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    290\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/base.py\u001b[0m in \u001b[0;36mdecision_function\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mn_features\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m             raise ValueError(\"X has %d features per sample; expecting %d\"\n\u001b[0;32m--> 270\u001b[0;31m                              % (X.shape[1], n_features))\n\u001b[0m\u001b[1;32m    271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m         scores = safe_sparse_dot(X, self.coef_.T,\n",
      "\u001b[0;31mValueError\u001b[0m: X has 3 features per sample; expecting 247"
     ]
    }
   ],
   "source": [
    "print(\"First 10 Testing Predictions: \")\n",
    "print(\"[Prob = 1\", \"Prob = 0]\")      \n",
    "print(lasso_model.predict_proba(X_train)[0:10])\n",
    "print('Mean Testing Accuracy: ', lasso_model.score(X_test,Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001 : 0.9073506891271057\n",
      "0.01 : 0.9104134762633997\n",
      "0.1 : 0.9287901990811639\n",
      "1 : 0.9624808575803981\n",
      "10 : 0.9578866768759571\n",
      "100 : 0.9387442572741195\n"
     ]
    }
   ],
   "source": [
    "#L1 model, train data,\n",
    "\n",
    "# List to maintain the different Mean Testing Accuracy (scores)\n",
    "scores = [] \n",
    "  \n",
    "# List to maintain the different values of lambda\n",
    "lambda_ = [0.001,0.01,0.1,1,10,100] \n",
    "#c=1/lambda_\n",
    "c=[1000,100,10,1,0.1,0.01]\n",
    "     \n",
    "\n",
    "# Loop to compute the different values of scores\n",
    "for i in range(6): \n",
    "    lasso_model = LogisticRegression(penalty=\"l1\",C=c[i],solver='liblinear')\n",
    "    lasso_model.fit(X_train, Y_train)\n",
    "    score = lasso_model.score(X_test,Y_test)\n",
    "    scores.append(score) \n",
    "  \n",
    "# Loop to print the different values of scores \n",
    "for i in range(0, len(lambda_)): \n",
    "    print(str(lambda_[i])+' : '+str(scores[i])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001 : 0.8966309341500766\n",
      "0.01 : 0.900459418070444\n",
      "0.1 : 0.9157733537519143\n",
      "1 : 0.9586523736600306\n",
      "10 : 0.9571209800918836\n",
      "100 : 0.9402756508422665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "#L1 model, validation data\n",
    "\n",
    "# List to maintain the different scores \n",
    "scores = [] \n",
    "  \n",
    "# List to maintain the different values of lambda\n",
    "lambda_ = [0.001,0.01,0.1,1,10,100] \n",
    "#c=1/lambda_\n",
    "c=[1000,100,10,1,0.1,0.01]\n",
    "\n",
    "\n",
    "  \n",
    "# Loop to compute the different values of  scores \n",
    "for i in range(6): \n",
    "    lasso_model = LogisticRegression(penalty=\"l1\",C=c[i],solver='liblinear')\n",
    "    lasso_model.fit(X_val, Y_val)\n",
    "    score = lasso_model.score(X_test,Y_test)\n",
    "    scores.append(score) \n",
    "  \n",
    "# Loop to print the different values of scores \n",
    "for i in range(0, len(lambda_)): \n",
    "    print(str(lambda_[i])+' : '+str(scores[i])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001 : 0.9119448698315467\n",
      "0.01 : 0.9173047473200613\n",
      "0.1 : 0.9387442572741195\n",
      "1 : 0.9594180704441041\n",
      "10 : 0.9693721286370597\n",
      "100 : 0.9532924961715161\n"
     ]
    }
   ],
   "source": [
    "#L2 model, train data\n",
    "\n",
    "# List to maintain the different scores \n",
    "scores = [] \n",
    "  \n",
    "# List to maintain the different values of lambda\n",
    "lambda_ = [0.001,0.01,0.1,1,10,100] \n",
    "#c=1/lambda_\n",
    "c=[1000,100,10,1,0.1,0.01]\n",
    "  \n",
    "\n",
    "# Loop to compute the different values of scores \n",
    "for i in range(6): \n",
    "    rigid_model = LogisticRegression(penalty=\"l2\",C=c[i],solver='liblinear')\n",
    "    rigid_model.fit(X_train, Y_train)\n",
    "    score = rigid_model.score(X_test,Y_test)\n",
    "    scores.append(score) \n",
    "  \n",
    "# Loop to print the different values of scores \n",
    "for i in range(0, len(lambda_)): \n",
    "    print(str(lambda_[i])+' : '+str(scores[i])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001 : 0.9035222052067381\n",
      "0.01 : 0.9073506891271057\n",
      "0.1 : 0.9218989280245024\n",
      "1 : 0.9548238897396631\n",
      "10 : 0.9624808575803981\n",
      "100 : 0.9532924961715161\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "#L2 model, validation data\n",
    "\n",
    "# List to maintain the different scores \n",
    "scores = [] \n",
    "  \n",
    "# List to maintain the different values of lambda \n",
    "lambda_ = [0.001,0.01,0.1,1,10,100] \n",
    "#c=1/lambda_\n",
    "c=[1000,100,10,1,0.1,0.01]\n",
    "\n",
    "\n",
    "# Loop to compute the different values of scores \n",
    "for i in range(6): \n",
    "    rigid_model = LogisticRegression(penalty=\"l2\",C=c[i],solver='liblinear')\n",
    "    rigid_model.fit(X_val, Y_val)\n",
    "    score = rigid_model.score(X_test,Y_test)\n",
    "    scores.append(score) \n",
    "  \n",
    "# Loop to print the different values of scores \n",
    "for i in range(0, len(lambda_)): \n",
    "    print(str(lambda_[i])+' : '+str(scores[i])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elastic net model, train data:\n",
      "0.001 : 0.9632465543644717\n",
      "0.01 : 0.9632465543644717\n",
      "0.1 : 0.9632465543644717\n",
      "1 : 0.9632465543644717\n",
      "10 : 0.9578866768759571\n",
      "100 : 0.94104134762634\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "# Elastic net model, train data\n",
    "\n",
    "# List to maintain the different scores \n",
    "scores = [] \n",
    "  \n",
    "# List to maintain the different values of lambda\n",
    "lambda_ = [0.001,0.01,0.1,1,10,100] \n",
    "#c=1/lambda_\n",
    "c=[1000,100,10,1,0.1,0.01]\n",
    "  \n",
    "\n",
    "# Loop to compute the different values of scores \n",
    "for i in range(6): \n",
    "    en_model = LogisticRegression(penalty=\"elasticnet\",C=c[i],solver='saga', l1_ratio=0.5)\n",
    "    en_model.fit(X_train, Y_train)\n",
    "    score = en_model.score(X_test,Y_test)\n",
    "    scores.append(score) \n",
    "  \n",
    "# Loop to print the different values of scores \n",
    "print(\"Elastic net model, train data:\")\n",
    "for i in range(0, len(lambda_)): \n",
    "    print(str(lambda_[i])+' : '+str(scores[i])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elastic net model, validation data:\n",
      "0.001 : 0.9594180704441041\n",
      "0.01 : 0.9594180704441041\n",
      "0.1 : 0.9594180704441041\n",
      "1 : 0.9594180704441041\n",
      "10 : 0.9532924961715161\n",
      "100 : 0.9418070444104135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "# Elastic net model, validation data\n",
    "\n",
    "# List to maintain the different scores \n",
    "scores = [] \n",
    "  \n",
    "# List to maintain the different values of lambda\n",
    "lambda_ = [0.001,0.01,0.1,1,10,100] \n",
    "#c=1/lambda_\n",
    "c=[1000,100,10,1,0.1,0.01]\n",
    "  \n",
    "\n",
    "# Loop to compute the different values of scores \n",
    "for i in range(6): \n",
    "    en_model = LogisticRegression(penalty=\"elasticnet\",C=c[i],solver='saga', l1_ratio=0.5)\n",
    "    en_model.fit(X_val, Y_val)\n",
    "    score = en_model.score(X_test,Y_test)\n",
    "    scores.append(score) \n",
    "  \n",
    "# Loop to print the different values of scores \n",
    "print(\"Elastic net model, validation data:\")\n",
    "for i in range(0, len(lambda_)): \n",
    "    print(str(lambda_[i])+' : '+str(scores[i])) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To summarize the test scores, L1 model, train data:\n",
    "0.001 : 0.9073506891271057\n",
    "\n",
    "0.01 : 0.9104134762633997\n",
    "\n",
    "0.1 : 0.9287901990811639\n",
    "\n",
    "1 : 0.9624808575803981\n",
    "\n",
    "10 : 0.9578866768759571\n",
    "\n",
    "100 : 0.9387442572741195\n",
    "\n",
    "\n",
    "#### L1 model, val data:\n",
    "0.001 : 0.8966309341500766\n",
    "\n",
    "0.01 : 0.9012251148545176\n",
    "\n",
    "0.1 : 0.9142419601837672\n",
    "\n",
    "1 : 0.9586523736600306\n",
    "\n",
    "10 : 0.9571209800918836\n",
    "\n",
    "100 : 0.9402756508422665\n",
    "\n",
    "#### L2 model, train data:\n",
    "0.001 : 0.9119448698315467\n",
    "\n",
    "0.01 : 0.9173047473200613\n",
    "\n",
    "0.1 : 0.9387442572741195\n",
    "\n",
    "1 : 0.9594180704441041\n",
    "\n",
    "10 : 0.9693721286370597\n",
    "\n",
    "100 : 0.9532924961715161\n",
    "\n",
    "#### L2 model. val data:\n",
    "0.001 : 0.9035222052067381\n",
    "\n",
    "0.01 : 0.9073506891271057\n",
    "\n",
    "0.1 : 0.9218989280245024\n",
    "\n",
    "1 : 0.9548238897396631\n",
    "\n",
    "10 : 0.9624808575803981\n",
    "\n",
    "100 : 0.9532924961715161\n",
    "\n",
    "#### Elastic net model, train data:\n",
    "0.001 : 0.9632465543644717\n",
    "\n",
    "0.01 : 0.9632465543644717\n",
    "\n",
    "0.1 : 0.9632465543644717\n",
    "\n",
    "1 : 0.9632465543644717\n",
    "\n",
    "10 : 0.9578866768759571\n",
    "\n",
    "100 : 0.94104134762634\n",
    "\n",
    "#### Elastic net model, val data:\n",
    "0.001 : 0.9594180704441041\n",
    "\n",
    "0.01 : 0.9594180704441041\n",
    "\n",
    "0.1 : 0.9594180704441041\n",
    "\n",
    "1 : 0.9594180704441041\n",
    "\n",
    "10 : 0.9532924961715161\n",
    "\n",
    "100 : 0.9418070444104135"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In L1 model, when $\\lambda=1$, the test score is the highest.\n",
    "\n",
    "In L2 model, when $\\lambda=10$, the test score is the highest.\n",
    "\n",
    "In Elastic net model, the test scores are the same from $lambda= 0.001$ to $1$, then decreases from $lambda = 1$ to $100$.\n",
    "\n",
    "I choose $\\lambda=1$ to be the most appropriate one because it brings the highest test scores in both L1 and Elastic Net model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Examine the coefficients for the optimal $\\lambda$ values for L1, L2, and elastic net models and explain the results. Also explain the change in the coefficients as you increase and decrease regularization $\\lambda$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=0.5, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='elasticnet',\n",
       "                   random_state=None, solver='saga', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# L1 model, train data\n",
    "lasso_model_train = LogisticRegression(penalty=\"l1\",C=1/1,solver='liblinear')\n",
    "lasso_model_train.fit(X_train, Y_train)\n",
    "\n",
    "# L1 model, validation data\n",
    "lasso_model_val = LogisticRegression(penalty=\"l1\",C=1/1,solver='liblinear')\n",
    "lasso_model_val.fit(X_val, Y_val)\n",
    "\n",
    "# L2 model, train data\n",
    "rigid_model_train = LogisticRegression(penalty=\"l2\",C=1/1,solver='liblinear')\n",
    "rigid_model_train.fit(X_train, Y_train)\n",
    "\n",
    "# L2 model, validation data\n",
    "rigid_model_val = LogisticRegression(penalty=\"l2\",C=1/1,solver='liblinear')\n",
    "rigid_model_val.fit(X_val, Y_val)\n",
    "\n",
    "# Elastic net model, train data\n",
    "en_model_train = LogisticRegression(penalty=\"elasticnet\",C=1/1,solver='saga', l1_ratio=0.5)\n",
    "en_model_train.fit(X_train, Y_train)\n",
    "\n",
    "# Elastic net model, validation data\n",
    "en_model_val = LogisticRegression(penalty=\"elasticnet\",C=1/1,solver='saga', l1_ratio=0.5)\n",
    "en_model_val.fit(X_val, Y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lasso_model_train_data</th>\n",
       "      <th>lasso_model_val_data</th>\n",
       "      <th>rigid_model_train_data</th>\n",
       "      <th>rigid_model_val_data</th>\n",
       "      <th>en_model_train_data</th>\n",
       "      <th>en_model_val_data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.919052</td>\n",
       "      <td>0.254845</td>\n",
       "      <td>0.980564</td>\n",
       "      <td>0.388138</td>\n",
       "      <td>0.298019</td>\n",
       "      <td>0.257874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.034757</td>\n",
       "      <td>0.009936</td>\n",
       "      <td>0.155853</td>\n",
       "      <td>0.136207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.346063</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.548276</td>\n",
       "      <td>0.060390</td>\n",
       "      <td>-0.000316</td>\n",
       "      <td>-0.033109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.084621</td>\n",
       "      <td>0.046905</td>\n",
       "      <td>-0.323843</td>\n",
       "      <td>-0.002755</td>\n",
       "      <td>-0.030128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.234813</td>\n",
       "      <td>0.112742</td>\n",
       "      <td>0.038305</td>\n",
       "      <td>0.028795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.288571</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.349488</td>\n",
       "      <td>-0.168655</td>\n",
       "      <td>0.024994</td>\n",
       "      <td>-0.003356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.383708</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.525458</td>\n",
       "      <td>0.078480</td>\n",
       "      <td>0.067973</td>\n",
       "      <td>0.041997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.314350</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.402943</td>\n",
       "      <td>-0.024260</td>\n",
       "      <td>0.154090</td>\n",
       "      <td>0.065452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.034377</td>\n",
       "      <td>0.078470</td>\n",
       "      <td>0.111461</td>\n",
       "      <td>0.051444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.172893</td>\n",
       "      <td>0.377689</td>\n",
       "      <td>0.385158</td>\n",
       "      <td>0.527819</td>\n",
       "      <td>0.138500</td>\n",
       "      <td>0.181768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.234953</td>\n",
       "      <td>0.012377</td>\n",
       "      <td>-0.409657</td>\n",
       "      <td>0.055939</td>\n",
       "      <td>0.000293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.201376</td>\n",
       "      <td>0.152582</td>\n",
       "      <td>0.375476</td>\n",
       "      <td>0.348702</td>\n",
       "      <td>0.069226</td>\n",
       "      <td>0.056828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.331470</td>\n",
       "      <td>0.286989</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>-0.141523</td>\n",
       "      <td>0.157103</td>\n",
       "      <td>-0.323984</td>\n",
       "      <td>0.289849</td>\n",
       "      <td>-0.000997</td>\n",
       "      <td>0.036521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.398805</td>\n",
       "      <td>-0.262801</td>\n",
       "      <td>-1.145264</td>\n",
       "      <td>-0.032584</td>\n",
       "      <td>-0.074255</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    lasso_model_train_data  lasso_model_val_data  rigid_model_train_data  \\\n",
       "1                 0.919052              0.254845                0.980564   \n",
       "2                 0.000000              0.000000               -0.034757   \n",
       "3                 0.346063              0.000000                0.548276   \n",
       "4                 0.000000             -0.084621                0.046905   \n",
       "5                 0.000000              0.000000               -0.234813   \n",
       "6                 0.288571              0.000000                0.349488   \n",
       "7                 0.383708              0.000000                0.525458   \n",
       "8                 0.314350              0.000000                0.402943   \n",
       "9                 0.000000              0.000000               -0.034377   \n",
       "10                0.172893              0.377689                0.385158   \n",
       "11                0.000000             -0.234953                0.012377   \n",
       "12                0.201376              0.152582                0.375476   \n",
       "13                0.000000              0.000000               -0.331470   \n",
       "14               -0.141523              0.157103               -0.323984   \n",
       "15                0.000000             -1.398805               -0.262801   \n",
       "\n",
       "    rigid_model_val_data  en_model_train_data  en_model_val_data  \n",
       "1               0.388138             0.298019           0.257874  \n",
       "2               0.009936             0.155853           0.136207  \n",
       "3               0.060390            -0.000316          -0.033109  \n",
       "4              -0.323843            -0.002755          -0.030128  \n",
       "5               0.112742             0.038305           0.028795  \n",
       "6              -0.168655             0.024994          -0.003356  \n",
       "7               0.078480             0.067973           0.041997  \n",
       "8              -0.024260             0.154090           0.065452  \n",
       "9               0.078470             0.111461           0.051444  \n",
       "10              0.527819             0.138500           0.181768  \n",
       "11             -0.409657             0.055939           0.000293  \n",
       "12              0.348702             0.069226           0.056828  \n",
       "13              0.286989             0.000000           0.015983  \n",
       "14              0.289849            -0.000997           0.036521  \n",
       "15             -1.145264            -0.032584          -0.074255  "
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First 15 Coefficients\n",
    "coef1 = lasso_model_train.coef_[0][0:15]\n",
    "coef2 = lasso_model_val.coef_[0][0:15]\n",
    "coef3 = rigid_model_train.coef_[0][0:15]\n",
    "coef4 = rigid_model_val.coef_[0][0:15]\n",
    "coef5 = en_model_train.coef_[0][0:15]\n",
    "coef6 = en_model_val.coef_[0][0:15]\n",
    "\n",
    "# Creates pandas DataFrame\n",
    "data = {'lasso_model_train_data':coef1, 'lasso_model_val_data':coef2, 'rigid_model_train_data':coef3, 'rigid_model_val_data':coef4, 'en_model_train_data':coef5, 'en_model_val_data':coef6} \n",
    "df = pd.DataFrame(data, index = list(range(1,16))) \n",
    "df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take the first 15 coefficients as a peak:\n",
    "\n",
    "Generally, most of the magnitude of coefficients in Rigid (L2) model are the largest compared to the coefficients from the other two model, and all the coefficients are non-zero values. Most of the coefficients in Lasso (L1) model are the second largest coefficients, some of the coefficients are 0 because the L1 regularization will shrink some parameters to zero. Coefficients from Elastic Net is somewhere between lasso and rigid. \n",
    "\n",
    "In L1 regression, some variables will not play any role in the model. L1 regression can be seen as a way to select features in a model. It penalizes the sum of absolute values of the coefficients. In other words, lasso can set some coefficients to zero, thus performing variable selection, while ridge regression cannot.\n",
    "\n",
    "Rigid (L2) will shrink the coefficients for least important predictors, very close to zero. But it will never make them exactly zero. In other words, the final model will include all predictors. \n",
    "\n",
    "In general, the bigger the penalization (larger $\\lambda$), the smaller the coefficients are."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Present and describe the results from the best performing logistic regression model, including sensitivity, specificity, a confusion matrix, accuracy, and F1 score on the test set. Why would accuracy not be a good metric if the data were imbalanced?\n",
    "\n",
    "### To choose the best model, first compare the obtained test scores from the three models specifically when $\\lambda=1$:\n",
    "\n",
    "#### L1 model, train data:\n",
    "1 : 0.9624808575803981\n",
    "\n",
    "#### L1 model, val data:\n",
    "1 : 0.9586523736600306\n",
    "\n",
    "#### L2 model, train data:\n",
    "1 : 0.9594180704441041\n",
    "\n",
    "#### L2 model. val data:\n",
    "1 : 0.9548238897396631\n",
    "\n",
    "#### Elastic net model, train data:\n",
    "1 : 0.9632465543644717\n",
    "\n",
    "#### Elastic net model, val data:\n",
    "1 : 0.9594180704441041\n",
    "\n",
    "But here, the test scores from Elastic net model are the largest compared to the other two, by comparing the models using train data and validation data separately.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Then compare sensitivity, specificity, a confusion matrix, accuracy, and F1 score from the three models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix : \n",
      " [[889   3]\n",
      " [  7 407]]\n",
      "Accuracy :  0.9923430321592649\n",
      "Sensitivity :  0.9966367713004485\n",
      "Specificity :  0.9830917874396136\n",
      "F1-score :  0.9944071588366891\n"
     ]
    }
   ],
   "source": [
    "#Building and training from the Rigid model. Using test data:\n",
    "model_test = LogisticRegression(penalty=\"l2\",C=1/1,solver='liblinear')\n",
    "fitted1 = model_test.fit(X_test, Y_test)\n",
    "\n",
    "###predicting values\n",
    "predicted_values1=fitted1.predict(X_test)\n",
    "predicted_values1[1:10]\n",
    "\n",
    "### Converting predicted values into classes using threshold\n",
    "threshold=0.8\n",
    "\n",
    "predicted_class1=np.zeros(predicted_values1.shape)\n",
    "predicted_class1[predicted_values1>threshold]=1\n",
    "predicted_class1\n",
    "\n",
    "#Confusion matrix, Accuracy, sensitivity and specificity\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm1 = confusion_matrix(Y_test,predicted_class1)\n",
    "print('Confusion Matrix : \\n', cm1)\n",
    "\n",
    "total1=sum(sum(cm1))\n",
    "#####from confusion matrix calculate accuracy\n",
    "accuracy1=(cm1[0,0]+cm1[1,1])/total1\n",
    "print ('Accuracy : ', accuracy1)\n",
    "\n",
    "sensitivity1 = cm1[0,0]/(cm1[0,0]+cm1[0,1])\n",
    "print('Sensitivity : ', sensitivity1 )\n",
    "\n",
    "specificity1 = cm1[1,1]/(cm1[1,0]+cm1[1,1])\n",
    "print('Specificity : ', specificity1)\n",
    "\n",
    "# precision:\n",
    "precision = cm1[0,0]/(cm1[0,0]+cm1[1,0])\n",
    "\n",
    "# recall:\n",
    "recall = cm1[0,0]/(cm1[0,0]+cm1[0,1])\n",
    "\n",
    "# F1 score:\n",
    "f1 = 2*precision*recall/(precision+recall)\n",
    "print('F1-score : ', f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix : \n",
      " [[888   4]\n",
      " [ 13 401]]\n",
      "Accuracy :  0.9869831546707504\n",
      "Sensitivity :  0.9955156950672646\n",
      "Specificity :  0.9685990338164251\n",
      "F1-score :  0.9905186837702176\n"
     ]
    }
   ],
   "source": [
    "#Building and training from the lasso model. Using test data:\n",
    "model_test = LogisticRegression(penalty=\"l1\",C=1/1,solver='liblinear')\n",
    "fitted1 = model_test.fit(X_test, Y_test)\n",
    "\n",
    "###predicting values\n",
    "predicted_values1=fitted1.predict(X_test)\n",
    "predicted_values1[1:10]\n",
    "\n",
    "### Converting predicted values into classes using threshold\n",
    "threshold=0.8\n",
    "\n",
    "predicted_class1=np.zeros(predicted_values1.shape)\n",
    "predicted_class1[predicted_values1>threshold]=1\n",
    "predicted_class1\n",
    "\n",
    "#Confusion matrix, Accuracy, sensitivity and specificity\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm1 = confusion_matrix(Y_test,predicted_class1)\n",
    "print('Confusion Matrix : \\n', cm1)\n",
    "\n",
    "total1=sum(sum(cm1))\n",
    "#####from confusion matrix calculate accuracy\n",
    "accuracy1=(cm1[0,0]+cm1[1,1])/total1\n",
    "print ('Accuracy : ', accuracy1)\n",
    "\n",
    "sensitivity1 = cm1[0,0]/(cm1[0,0]+cm1[0,1])\n",
    "print('Sensitivity : ', sensitivity1 )\n",
    "\n",
    "specificity1 = cm1[1,1]/(cm1[1,0]+cm1[1,1])\n",
    "print('Specificity : ', specificity1)\n",
    "\n",
    "# precision:\n",
    "precision = cm1[0,0]/(cm1[0,0]+cm1[1,0])\n",
    "\n",
    "# recall:\n",
    "recall = cm1[0,0]/(cm1[0,0]+cm1[0,1])\n",
    "\n",
    "# F1 score:\n",
    "f1 = 2*precision*recall/(precision+recall)\n",
    "print('F1-score : ', f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix : \n",
      " [[879  13]\n",
      " [ 32 382]]\n",
      "Accuracy :  0.9655436447166922\n",
      "Sensitivity :  0.9854260089686099\n",
      "Specificity :  0.9227053140096618\n",
      "F1-score :  0.9750415973377704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "#Building and training from the Elastic Net model. Using test data:\n",
    "model_test = LogisticRegression(penalty=\"elasticnet\",C=1/1,solver='saga', l1_ratio=0.5)\n",
    "fitted1 = model_test.fit(X_test, Y_test)\n",
    "\n",
    "###predicting values\n",
    "predicted_values1=fitted1.predict(X_test)\n",
    "predicted_values1[1:10]\n",
    "\n",
    "### Converting predicted values into classes using threshold\n",
    "threshold=0.8\n",
    "\n",
    "predicted_class1=np.zeros(predicted_values1.shape)\n",
    "predicted_class1[predicted_values1>threshold]=1\n",
    "predicted_class1\n",
    "\n",
    "#Confusion matrix, Accuracy, sensitivity and specificity\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm1 = confusion_matrix(Y_test,predicted_class1)\n",
    "print('Confusion Matrix : \\n', cm1)\n",
    "\n",
    "total1=sum(sum(cm1))\n",
    "#####from confusion matrix calculate accuracy\n",
    "accuracy1=(cm1[0,0]+cm1[1,1])/total1\n",
    "print ('Accuracy : ', accuracy1)\n",
    "\n",
    "sensitivity1 = cm1[0,0]/(cm1[0,0]+cm1[0,1])\n",
    "print('Sensitivity : ', sensitivity1 )\n",
    "\n",
    "specificity1 = cm1[1,1]/(cm1[1,0]+cm1[1,1])\n",
    "print('Specificity : ', specificity1)\n",
    "\n",
    "# precision:\n",
    "precision = cm1[0,0]/(cm1[0,0]+cm1[1,0])\n",
    "\n",
    "# recall:\n",
    "recall = cm1[0,0]/(cm1[0,0]+cm1[0,1])\n",
    "\n",
    "# F1 score:\n",
    "f1 = 2*precision*recall/(precision+recall)\n",
    "print('F1-score : ', f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the Rigid model generally has the best accuracy, sensitivity, specificity, and f1-score compared to the other two models. So Rigid instead of Elastic net model is the best performing logistic regression model.\n",
    "\n",
    "Additionally, rigid (L2) is the best logistic regression model for large set of parameters, because it works well if there are many large parameters of about the same value (ergo: when most predictors impact the response. Here in our data, we have 247 miRNA as parameters(predictors), so it satisfies that it has a large set of parameters.\n",
    "\n",
    "#### Why would accuracy not be a good metric if the data were imbalanced?\n",
    "\n",
    "Classification accuracy is a metric that summarizes the performance of a classification model as the number of correct predictions divided by the total number of predictions.\n",
    "\n",
    "A really high accuracy from an imbalanced data is called \"accuracy paradox\". When the class distribution is slightly skewed, accuracy can still be a useful metric. When the skew in the class distributions are severe, accuracy can become an unreliable measure of model performance because of the intuitions developed by practitioners on datasets with an equal class distribution.\n",
    "\n",
    "reference: https://machinelearningmastery.com/failure-of-accuracy-for-imbalanced-class-distributions/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Naïve Bayes classifier\n",
    "\n",
    "The Naïve Bayes algorithm is based on the Bayes Theorem for calculating probabilities and conditional probabilities to predict the class of unknown data sets. It assumes that the presence of a particular feature in a class is unrelated to the presence of any other feature, in other words, all of the properties independently contribute to the probability that an object is supposed to be a certain class. It computes $P(c|x)$ from $P(c)$, $P(x)$ and $P(x|c)$:\n",
    "\n",
    "$$P(c|x) = \\frac{P(x|c)P(c)}{P(x)}$$\n",
    "\n",
    "I start with build a dataframe with the data given from the pdf assignment file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Town</th>\n",
       "      <th>State</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Burlington</td>\n",
       "      <td>Vermont</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Rutland</td>\n",
       "      <td>Vermont</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Springfield</td>\n",
       "      <td>Vermont</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Springfield</td>\n",
       "      <td>New Hampshire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Lebanon</td>\n",
       "      <td>New Hampshire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>Hanover</td>\n",
       "      <td>New Hampshire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>White River Junction</td>\n",
       "      <td>Vermont</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>Hartford</td>\n",
       "      <td>Vermont</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>Shelburne</td>\n",
       "      <td>Vermont</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>Concord</td>\n",
       "      <td>New Hampshire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>Nashua</td>\n",
       "      <td>New Hampshire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>Lyme</td>\n",
       "      <td>New Hampshire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>Shelburne</td>\n",
       "      <td>New Hampshire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>Claremont</td>\n",
       "      <td>New Hampshire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>Springfield</td>\n",
       "      <td>New Hampshire</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Town          State\n",
       "0             Burlington        Vermont\n",
       "1                Rutland        Vermont\n",
       "2            Springfield        Vermont\n",
       "3            Springfield  New Hampshire\n",
       "4                Lebanon  New Hampshire\n",
       "5                Hanover  New Hampshire\n",
       "6   White River Junction        Vermont\n",
       "7               Hartford        Vermont\n",
       "8              Shelburne        Vermont\n",
       "9                Concord  New Hampshire\n",
       "10                Nashua  New Hampshire\n",
       "11                  Lyme  New Hampshire\n",
       "12             Shelburne  New Hampshire\n",
       "13             Claremont  New Hampshire\n",
       "14           Springfield  New Hampshire"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "town = ['Burlington','Rutland', 'Springfield', 'Springfield', 'Lebanon','Hanover','White River Junction', 'Hartford', 'Shelburne','Concord','Nashua','Lyme','Shelburne','Claremont','Springfield']\n",
    "state = ['Vermont','Vermont','Vermont','New Hampshire','New Hampshire','New Hampshire','Vermont','Vermont','Vermont','New Hampshire','New Hampshire','New Hampshire','New Hampshire','New Hampshire','New Hampshire']\n",
    "        \n",
    "data = {'Town': town, 'State':state} \n",
    "df = pd.DataFrame(data) \n",
    "df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$P(Vermont|Springfield) = \\frac{P(Springfield|Vermont) * P(Vermont)}{P (Springfield)}$\n",
    "\n",
    "Here we have $P(Springfield|Vermont) = \\frac{1}{6}$, $P(Springfield) = \\frac{3}{15} = \\frac{1}{5}$, $P(Vermont)= \\frac{2}{5}$, \n",
    "\n",
    "so $P(Vermont|Springfield) =\\frac{ \\frac{1}{6} \\frac{2}{5}}{\\frac{1}{5}} = \\frac{1}{3}$, which means that the probability of Springfield in Vermont is $\\frac{1}{3}$, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state: [1 0 1 0 0]\n"
     ]
    }
   ],
   "source": [
    "# prepare data\n",
    "#town1 = ['Burlington', 'Springfield', 'White River Junction','Concord','Shelburne']\n",
    "#town2 = ['Rutland', 'Lebanon', 'Hartford','Nashua','Claremont']\n",
    "#town3 = ['Springfield','Hanover','Shelburne','Lyme','Springfield']\n",
    "\n",
    "\n",
    "    #'Burlington=1', 'Springfield=2', 'White River Junction=3','Concord=4','Shelburne=5'\n",
    "    #'Rutland=6', 'Lebanon=7', 'Hartford=8','Nashua=9','Claremont=10'\n",
    "    # 'Springfield=2','Hanover=11','Shelburne=5','Lyme=12','Springfield=2'\n",
    "\n",
    "# Generate train data:   \n",
    "#town1 = np.array([1,2,3,4,5])    \n",
    "#town2 = np.array([6,7,8,9,10])   \n",
    "#town3 = np.array([2,11,5,12,2])\n",
    "\n",
    "feature1 = ([[1,6,2],[2,7,11],[3,8,5],[4,9,12],[5,10,2]])\n",
    "\n",
    "# Import LabelEncoder\n",
    "from sklearn import preprocessing\n",
    "#creating labelEncoder\n",
    "le = preprocessing.LabelEncoder()\n",
    "# Converting string labels into numbers.\n",
    "state = ['Vermont','New Hampshire','Vermont','New Hampshire','New Hampshire']\n",
    "label=le.fit_transform(state)\n",
    "print(\"state:\",label) # 1 is Vermont, 0 is New Hampshire\n",
    "\n",
    "\n",
    "# Generate test data: Springfield, Shelburne, Manchester\n",
    "# Springfield = 2; Shelburne = 5; Manchester not exist, thus set Manchester = 13 which doesn't exist in the town dataset.\n",
    "X_test = np.array([[2,5,13]])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Springfield, Shelburne, Manchester:  [0]\n",
      "Predicted Burlington, White River Junction, Hartford: [1]\n"
     ]
    }
   ],
   "source": [
    "# Import Gaussian Naive Bayes model\n",
    "# Sklearn applies Laplace smoothing by default when you train a Naive Bayes classifier.\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "#Create a Gaussian Classifier\n",
    "model = GaussianNB()\n",
    "\n",
    "# Train the model using the training sets\n",
    "model.fit(feature1,label)\n",
    "\n",
    "#Predict Output\n",
    "y_pred = model.fit(feature1,label).predict(X_test) \n",
    "print('Predicted Springfield, Shelburne, Manchester: ', y_pred)\n",
    "\n",
    "# just try a random set of towns that are only in Vermont to see what the predicted results are:\n",
    "y_pred2= model.predict([[1,3,8]]) # Burlington=1; White River Junction=3; Hartford=8\n",
    "print(\"Predicted Burlington, White River Junction, Hartford:\", y_pred2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since both of Springfield and Shelburne are predicted to be 0, meaning in New Hampshire, we can see that the 6th set of towns - Springfield, Shelburne, Manchester,  are in New Hampshire. Although Manchester is predicted to be in Vermont, we still follow the results of the two existed towns in the train data. \n",
    "\n",
    "Again, by using Gaussian Naive Bayes, the built in algorithm in sklearn takes care of the laplace smoothing automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Prob = 0 Prob = 1]\n",
      "[[5.50006370e-03 9.94499936e-01]\n",
      " [9.99917627e-01 8.23732135e-05]\n",
      " [4.51469972e-01 5.48530028e-01]\n",
      " [9.99999991e-01 9.07796649e-09]\n",
      " [9.98036558e-01 1.96344195e-03]]\n"
     ]
    }
   ],
   "source": [
    "print(\"[Prob = 0\", \"Prob = 1]\") \n",
    "print(model.predict_proba(feature1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From above, we can see that the probability of the first set of town in New Hampshire is 5.50006370e-03, in Vermont is 9.94499936e-01.\n",
    "\n",
    "The probability of the second set of town in New Hampshire is 9.99917627e-01, in Vermont is 8.23732135e-05, etc.\n",
    "\n",
    "The estimted result could be problematic since it would give us probability 0 for documents with unknown town (Manchester). We use laplace smoothing (GaussianNB above) to solve the problem.\n",
    "\n",
    "Setting $\\alpha=1$ other other value is called Laplace smoothing.\n",
    "Let's see how it works without laplace smoothing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1e-10, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "rng = np.random.RandomState(1)\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "model2 = MultinomialNB(alpha=1.0e-10, class_prior=None, fit_prior=True) \n",
    "# didn't set alpha to 0 because:\n",
    "# \"UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\"\n",
    "model2.fit(feature1,label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Springfield: [0]\n"
     ]
    }
   ],
   "source": [
    "#Predict Output\n",
    "predicted1= model2.predict(X_test)\n",
    "print(\"Predicted Springfield:\", predicted1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From above, we get the same predicted results compared to Gaussian Naive Bayes with laplace smoothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Prob = 0 Prob = 1]\n",
      "[[0.37822373 0.62177627]\n",
      " [0.93082248 0.06917752]\n",
      " [0.55608126 0.44391874]\n",
      " [0.93036734 0.06963266]\n",
      " [0.22433114 0.77566886]]\n"
     ]
    }
   ],
   "source": [
    "print(\"[Prob = 0\", \"Prob = 1]\") \n",
    "print(model2.predict_proba(feature1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But the probability of the towns in the test set with respect to each class without laplace smoothing is different compared to with laplace smoothing.\n",
    "\n",
    "Specifically, the third set of town should've been as Vermont, which is correctly predicted in GaussianNB model, but here without laplace smoothing, the third set of town is predicted to be more likely in New Hampshire. Similar opposite result appears in the 5th set of town."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
